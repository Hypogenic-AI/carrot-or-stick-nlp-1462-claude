\section{Discussion}
\label{sec:discussion}

\subsection{Reconciling the Literature}
\label{sec:reconciliation}

Our results provide a unified explanation for the contradictory findings in prior work.
\tabref{tab:reconciliation} maps each published finding to our experimental evidence.

\begin{table}[t]
    \centering
    \small
    \caption{Reconciliation of prior findings with our results. Each contradiction in the literature can be attributed to model heterogeneity, dataset scale, or evaluation confounds.}
    \label{tab:reconciliation}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}p{4cm}p{4cm}p{5cm}@{}}
        \toprule
        \textbf{Published finding} & \textbf{Our explanation} & \textbf{Supporting evidence} \\
        \midrule
        \citet{yin2024should}: impolite prompts hurt performance & Tested older models (GPT-3.5/4, Llama2) with different RLHF tuning & Our \gptfour shows no consistent penalty for rudeness \\
        \cmidrule(lr){1-3}
        \citet{dobariya2025mind}: rude prompts outperform polite & Small dataset (50 questions) amplifies noise; directionally consistent with Claude & Our \claudesonnet shows +12.2\pp for rude over neutral on STEM \\
        \cmidrule(lr){1-3}
        \citet{cai2025does}: effects non-significant at scale & Tested GPT-4o mini + Gemini + Llama with large datasets & Our \gptfour and \geminiflash results confirm $\pm$1--3\pp effects \\
        \cmidrule(lr){1-3}
        \citet{li2023emotionprompt}: positive stimuli improve by 8--115\% & Different evaluation methods and older models & Minimal effect on \gptfour and \geminiflash; format disruption on \claudesonnet \\
        \cmidrule(lr){1-3}
        \citet{wang2024negativeprompt}: negative stimuli improve by 12.89\% & Tested on Instruction Induction with older models & $\leq$1\pp effect in our MCQ evaluation \\
        \bottomrule
    \end{tabular}
    }
\end{table}

The root causes of disagreement are:
\begin{enumerate}[leftmargin=*,itemsep=0pt,topsep=2pt]
    \item \textbf{Model heterogeneity.} Different models have radically different tone sensitivity due to different alignment training. \claudesonnet and \gptfour differ by an order of magnitude in their response to the same tone manipulation.
    \item \textbf{Dataset scale.} Small datasets (50--100 questions) amplify random variation into apparently significant tone effects. Our results on 200 questions per domain show that most effects are $\leq$3\pp for two of three models, consistent with \citet{cai2025does}.
    \item \textbf{Evaluation confounds.} Emotional suffixes can disrupt instruction following, creating misleading accuracy drops. Our \claudesonnet \emotionpos finding illustrates how format compliance issues can be mistaken for reasoning failures.
\end{enumerate}

\subsection{Why Is \claudesonnet Uniquely Sensitive?}
\label{sec:claude_sensitivity}

\claudesonnet stands out as the only model with large, consistent tone effects.
We offer two possible explanations.
First, Claude's alignment training may place greater weight on conversational context, causing tone signals to modulate the model's interpretation of the task.
Commanding prompts may signal a high-stakes, precision-oriented context that focuses the model on concise, accurate responses, while polite framing may signal a more open-ended conversational context.
Second, Claude's instruction-following behavior appears more sensitive to trailing context: the \emotionpos suffix, placed \emph{after} the question, effectively overrides the earlier instruction to respond with only a letter.
\gptfour and \geminiflash show no such override behavior, suggesting differences in how these models weigh positional information.

\subsection{Practical Implications}
\label{sec:practical}

For practitioners, our results yield clear guidance:

\para{For most models, tone does not matter.}
\gptfour and \geminiflash are robust to tone variation.
Users can write in whatever style is natural without worrying about accuracy effects.

\para{For Claude, prefer concise and direct prompts.}
On factual tasks, commanding prompts outperform polite ones by a meaningful margin.
This does not mean users should be gratuitously rude, but rather that unnecessary courtesy phrases may slightly dilute performance.

\para{Avoid emotional suffixes.}
Appending motivational phrases to prompts provides no consistent benefit across models and can actively disrupt instruction following in Claude.
If emotional framing is desired, negative stimuli are safer than positive ones, as they did not trigger format disruption in any tested model.

\para{Focus on clarity, not tone.}
The maximum tone effect we observe for the two most robust models is $\pm$3\pp.
Investing prompt engineering effort in task clarity, few-shot examples, or structured output formats will yield substantially larger gains than tone optimization.

\subsection{Limitations}
\label{sec:limitations}

\para{Token limit constraint.}
Our strict \texttt{max\_tokens}~=~5 setting may penalize verbose models disproportionately.
The \claudesonnet \emotionpos catastrophe is partially an artifact of this constraint; with a higher token limit, the model would likely emit the correct answer after its preamble.
Future work should test with higher token limits and post-hoc answer extraction.

\para{Limited trial variation.}
With temperature = 0, most conditions produced identical results across all three trials, inflating $F$-statistics and $\eta^2$ values in the ANOVA.
More informative variance would come from different question subsets or bootstrapped samples.

\para{No prompt length control.}
Polite prompts are longer than neutral ones by 15--30 tokens.
We did not pad shorter prompts to match, so some observed effect could stem from prompt length rather than tone.

\para{English only.}
\citet{yin2024should} showed that politeness effects are language-dependent, with different optimal levels for English, Chinese, and Japanese.
Our results may not generalize to other languages.

\para{Three models only.}
We do not test open-source models (e.g., Llama, Mistral) or smaller model variants.
The finding that tone sensitivity is model-dependent implies that results should not be extrapolated to untested models.
