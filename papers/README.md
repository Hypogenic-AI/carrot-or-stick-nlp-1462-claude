# Downloaded Papers

## Core Politeness/Tone Studies

1. **Should We Respect LLMs? A Cross-Lingual Study on the Influence of Prompt Politeness on LLM Performance**
   - File: `2402.14531_should_we_respect_llms.pdf`
   - Authors: Ziqi Yin, Hao Wang, Kaito Horio, Daisuke Kawahara, Satoshi Sekine
   - Year: 2024
   - arXiv: 2402.14531
   - Why relevant: Foundational cross-lingual study (EN/CN/JP) on politeness effects. 8 politeness levels, multiple models and tasks. Created JMMLU benchmark.

2. **Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy**
   - File: `2510.04950_mind_your_tone.pdf`
   - Authors: Om Dobariya, Akhil Kumar
   - Year: 2025
   - arXiv: 2510.04950
   - Why relevant: Contradictory finding — rude prompts outperform polite ones on GPT-4o (84.8% vs 80.8%).

3. **Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA**
   - File: `2512.12812_does_tone_change_answer.pdf`
   - Authors: Hanyu Cai, Binqi Shen, Lier Jin, Lan Hu, Xiaojing Fan
   - Year: 2025
   - arXiv: 2512.12812
   - Why relevant: Largest cross-model study (3 LLMs, MMMLU). Shows tone effects diminish at scale and are domain-specific.

## Emotional Stimuli Studies

4. **Large Language Models Understand and Can Be Enhanced by Emotional Stimuli (EmotionPrompt)**
   - File: `2307.11760_emotionprompt.pdf`
   - Authors: Cheng Li, Jindong Wang, et al.
   - Year: 2023
   - arXiv: 2307.11760
   - Why relevant: Seminal "carrot" paper. 11 positive emotional stimuli improve LLM performance (8% on Instruction Induction, 115% on BIG-Bench).

5. **NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli**
   - File: `2405.02814_negativeprompt.pdf`
   - Authors: Xu Wang, Cheng Li, Yi Chang, Jindong Wang, Yuan Wu
   - Year: 2024
   - arXiv: 2405.02814
   - Why relevant: "Stick" counterpart to EmotionPrompt. 10 negative stimuli yield 12.89% improvement (higher than positive stimuli on some tasks).

## Prompt Engineering and Sensitivity

6. **Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4**
   - File: `2312.16171_principled_instructions.pdf`
   - Authors: Sondos Mahmoud Bsharat, Aidar Myrzakhan, Zhiqiang Shen
   - Year: 2023
   - arXiv: 2312.16171
   - Why relevant: 26 prompting principles. Principle 1: "No need to be polite with LLM" — removing politeness improved results ~5%.

7. **Prompt Sentiment: The Catalyst for LLM Change**
   - File: `2503.13510_prompt_sentiment_catalyst.pdf`
   - Authors: Vishal Gandhi, Sagar Gandhi
   - Year: 2025
   - arXiv: 2503.13510
   - Why relevant: Systematic study of sentiment effects across 5 LLMs and 6 domains. Negative sentiment reduces factual accuracy ~8.4%.

8. **Benchmarking Prompt Sensitivity in Large Language Models**
   - File: `2502.06065_benchmarking_prompt_sensitivity.pdf`
   - Authors: Amirhossein Razavi et al.
   - Year: 2025
   - arXiv: 2502.06065
   - Why relevant: Introduces PromptSET dataset for studying prompt sensitivity. Shows existing methods struggle with sensitivity prediction.

## Threat/Manipulation Studies

9. **Analysis of Threat-Based Manipulation in Large Language Models**
   - File: `2507.21133_threat_based_manipulation.pdf`
   - Year: 2025
   - arXiv: 2507.21133
   - Why relevant: Tests extreme "stick" (threats) on Claude, GPT-4, Gemini. Shows threats can paradoxically enhance performance.

## Prompt Sensitivity Meta-Analysis

10. **Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs**
    - File: `2509.01790_flaw_or_artifact.pdf`
    - Year: 2025
    - arXiv: 2509.01790
    - Why relevant: Argues prompt sensitivity is partly an artifact of heuristic evaluation, not an inherent LLM flaw.

## Persona/Role Studies

11. **When "A Helpful Assistant" Is Not Really Helpful: Personas in System Prompts Do Not Improve Performances of Large Language Models**
    - File: `2311.10054_helpful_assistant_personas.pdf`
    - Authors: Zheng et al.
    - Year: 2023
    - arXiv: 2311.10054
    - Why relevant: Shows system prompt personas don't improve objective task performance. Related to tone/framing effects.
