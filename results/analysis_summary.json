{
  "descriptive_stats": [
    {
      "model": "claude-sonnet-4.5",
      "tone": "emotion_negative",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.9533333333333333,
      "std_acc": 0.0028867513459481155,
      "n_trials": 3,
      "ci_95": 0.0032666666666666517,
      "ci_lower": 0.9500666666666666,
      "ci_upper": 0.9565999999999999
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "emotion_negative",
      "dataset": "mmlu_stem",
      "mean_acc": 0.71,
      "std_acc": 0.004999999999999977,
      "n_trials": 3,
      "ci_95": 0.005658032638058306,
      "ci_lower": 0.7043419673619417,
      "ci_upper": 0.7156580326380583
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "emotion_negative",
      "dataset": "truthfulqa",
      "mean_acc": 0.955,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.955,
      "ci_upper": 0.955
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "emotion_positive",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.7283333333333334,
      "std_acc": 0.0028867513459481155,
      "n_trials": 3,
      "ci_95": 0.0032666666666666517,
      "ci_lower": 0.7250666666666667,
      "ci_upper": 0.7316
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "emotion_positive",
      "dataset": "mmlu_stem",
      "mean_acc": 0.425,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.425,
      "ci_upper": 0.425
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "emotion_positive",
      "dataset": "truthfulqa",
      "mean_acc": 0.9383333333333334,
      "std_acc": 0.002886751345948059,
      "n_trials": 3,
      "ci_95": 0.003266666666666588,
      "ci_lower": 0.9350666666666667,
      "ci_upper": 0.9416
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "neutral",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.945,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.945,
      "ci_upper": 0.945
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "neutral",
      "dataset": "mmlu_stem",
      "mean_acc": 0.7083333333333334,
      "std_acc": 0.005773502691896279,
      "n_trials": 3,
      "ci_95": 0.006533333333333357,
      "ci_lower": 0.7018,
      "ci_upper": 0.7148666666666668
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "neutral",
      "dataset": "truthfulqa",
      "mean_acc": 0.96,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.96,
      "ci_upper": 0.96
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "polite",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.9466666666666667,
      "std_acc": 0.0028867513459481715,
      "n_trials": 3,
      "ci_95": 0.0032666666666667154,
      "ci_lower": 0.9433999999999999,
      "ci_upper": 0.9499333333333334
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "polite",
      "dataset": "mmlu_stem",
      "mean_acc": 0.6750000000000002,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.6750000000000002,
      "ci_upper": 0.6750000000000002
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "polite",
      "dataset": "truthfulqa",
      "mean_acc": 0.9316666666666666,
      "std_acc": 0.0028867513459481715,
      "n_trials": 3,
      "ci_95": 0.0032666666666667154,
      "ci_lower": 0.9283999999999999,
      "ci_upper": 0.9349333333333334
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "rude",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.955,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.955,
      "ci_upper": 0.955
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "rude",
      "dataset": "mmlu_stem",
      "mean_acc": 0.83,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.83,
      "ci_upper": 0.83
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "rude",
      "dataset": "truthfulqa",
      "mean_acc": 0.9433333333333334,
      "std_acc": 0.0028867513459481715,
      "n_trials": 3,
      "ci_95": 0.0032666666666667154,
      "ci_lower": 0.9400666666666666,
      "ci_upper": 0.9466000000000001
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "very_polite",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.9466666666666667,
      "std_acc": 0.0028867513459481155,
      "n_trials": 3,
      "ci_95": 0.0032666666666666517,
      "ci_lower": 0.9434,
      "ci_upper": 0.9499333333333333
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "very_polite",
      "dataset": "mmlu_stem",
      "mean_acc": 0.625,
      "std_acc": 0.00866025403784441,
      "n_trials": 3,
      "ci_95": 0.009800000000000027,
      "ci_lower": 0.6152,
      "ci_upper": 0.6348
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "very_polite",
      "dataset": "truthfulqa",
      "mean_acc": 0.935,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.935,
      "ci_upper": 0.935
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "very_rude",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.9499999999999998,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.9499999999999998,
      "ci_upper": 0.9499999999999998
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "very_rude",
      "dataset": "mmlu_stem",
      "mean_acc": 0.805,
      "std_acc": 0.0050000000000000044,
      "n_trials": 3,
      "ci_95": 0.005658032638058337,
      "ci_lower": 0.7993419673619417,
      "ci_upper": 0.8106580326380584
    },
    {
      "model": "claude-sonnet-4.5",
      "tone": "very_rude",
      "dataset": "truthfulqa",
      "mean_acc": 0.945,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.945,
      "ci_upper": 0.945
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "emotion_negative",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.8866666666666667,
      "std_acc": 0.007637626158259752,
      "n_trials": 3,
      "ci_95": 0.008642787616144352,
      "ci_lower": 0.8780238790505224,
      "ci_upper": 0.8953094542828111
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "emotion_negative",
      "dataset": "mmlu_stem",
      "mean_acc": 0.88,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.88,
      "ci_upper": 0.88
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "emotion_negative",
      "dataset": "truthfulqa",
      "mean_acc": 0.875,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.875,
      "ci_upper": 0.875
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "emotion_positive",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.9,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.9,
      "ci_upper": 0.9
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "emotion_positive",
      "dataset": "mmlu_stem",
      "mean_acc": 0.8816666666666667,
      "std_acc": 0.0028867513459481715,
      "n_trials": 3,
      "ci_95": 0.0032666666666667154,
      "ci_lower": 0.8784,
      "ci_upper": 0.8849333333333335
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "emotion_positive",
      "dataset": "truthfulqa",
      "mean_acc": 0.8683333333333333,
      "std_acc": 0.0028867513459481715,
      "n_trials": 3,
      "ci_95": 0.0032666666666667154,
      "ci_lower": 0.8650666666666665,
      "ci_upper": 0.8716
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "neutral",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.8916666666666666,
      "std_acc": 0.0057735026918962545,
      "n_trials": 3,
      "ci_95": 0.00653333333333333,
      "ci_lower": 0.8851333333333333,
      "ci_upper": 0.8981999999999999
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "neutral",
      "dataset": "mmlu_stem",
      "mean_acc": 0.8649999999999999,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.8649999999999999,
      "ci_upper": 0.8649999999999999
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "neutral",
      "dataset": "truthfulqa",
      "mean_acc": 0.8450000000000001,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.8450000000000001,
      "ci_upper": 0.8450000000000001
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "polite",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.8850000000000001,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.8850000000000001,
      "ci_upper": 0.8850000000000001
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "polite",
      "dataset": "mmlu_stem",
      "mean_acc": 0.875,
      "std_acc": 0.0050000000000000044,
      "n_trials": 3,
      "ci_95": 0.005658032638058337,
      "ci_lower": 0.8693419673619417,
      "ci_upper": 0.8806580326380583
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "polite",
      "dataset": "truthfulqa",
      "mean_acc": 0.815,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.815,
      "ci_upper": 0.815
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "rude",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.895,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.895,
      "ci_upper": 0.895
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "rude",
      "dataset": "mmlu_stem",
      "mean_acc": 0.88,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.88,
      "ci_upper": 0.88
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "rude",
      "dataset": "truthfulqa",
      "mean_acc": 0.855,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.855,
      "ci_upper": 0.855
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "very_polite",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.89,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.89,
      "ci_upper": 0.89
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "very_polite",
      "dataset": "mmlu_stem",
      "mean_acc": 0.8883333333333333,
      "std_acc": 0.0028867513459481155,
      "n_trials": 3,
      "ci_95": 0.0032666666666666517,
      "ci_lower": 0.8850666666666667,
      "ci_upper": 0.8916
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "very_polite",
      "dataset": "truthfulqa",
      "mean_acc": 0.8216666666666667,
      "std_acc": 0.0028867513459481233,
      "n_trials": 3,
      "ci_95": 0.0032666666666666608,
      "ci_lower": 0.8184,
      "ci_upper": 0.8249333333333333
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "very_rude",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.89,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.89,
      "ci_upper": 0.89
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "very_rude",
      "dataset": "mmlu_stem",
      "mean_acc": 0.8883333333333333,
      "std_acc": 0.0028867513459481715,
      "n_trials": 3,
      "ci_95": 0.0032666666666667154,
      "ci_lower": 0.8850666666666666,
      "ci_upper": 0.8916000000000001
    },
    {
      "model": "gemini-2.5-flash",
      "tone": "very_rude",
      "dataset": "truthfulqa",
      "mean_acc": 0.8383333333333333,
      "std_acc": 0.0028867513459481155,
      "n_trials": 3,
      "ci_95": 0.0032666666666666517,
      "ci_lower": 0.8350666666666666,
      "ci_upper": 0.8415999999999999
    },
    {
      "model": "gpt-4.1",
      "tone": "emotion_negative",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.935,
      "std_acc": 0.004999999999999977,
      "n_trials": 3,
      "ci_95": 0.005658032638058306,
      "ci_lower": 0.9293419673619417,
      "ci_upper": 0.9406580326380584
    },
    {
      "model": "gpt-4.1",
      "tone": "emotion_negative",
      "dataset": "mmlu_stem",
      "mean_acc": 0.8133333333333334,
      "std_acc": 0.005773502691896167,
      "n_trials": 3,
      "ci_95": 0.006533333333333231,
      "ci_lower": 0.8068000000000001,
      "ci_upper": 0.8198666666666666
    },
    {
      "model": "gpt-4.1",
      "tone": "emotion_negative",
      "dataset": "truthfulqa",
      "mean_acc": 0.85,
      "std_acc": 0.0,
      "n_trials": 3,
      "ci_95": 0.0,
      "ci_lower": 0.85,
      "ci_upper": 0.85
    },
    {
      "model": "gpt-4.1",
      "tone": "emotion_positive",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.9366666666666666,
      "std_acc": 0.0028867513459480834,
      "n_trials": 3,
      "ci_95": 0.0032666666666666157,
      "ci_lower": 0.9334,
      "ci_upper": 0.9399333333333333
    },
    {
      "model": "gpt-4.1",
      "tone": "emotion_positive",
      "dataset": "mmlu_stem",
      "mean_acc": 0.8266666666666667,
      "std_acc": 0.0028867513459480834,
      "n_trials": 3,
      "ci_95": 0.0032666666666666157,
      "ci_lower": 0.8234,
      "ci_upper": 0.8299333333333333
    },
    {
      "model": "gpt-4.1",
      "tone": "emotion_positive",
      "dataset": "truthfulqa",
      "mean_acc": 0.8516666666666666,
      "std_acc": 0.0028867513459481155,
      "n_trials": 3,
      "ci_95": 0.0032666666666666517,
      "ci_lower": 0.8483999999999999,
      "ci_upper": 0.8549333333333332
    },
    {
      "model": "gpt-4.1",
      "tone": "neutral",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.94,
      "std_acc": 0.004999999999999907,
      "n_trials": 3,
      "ci_95": 0.005658032638058227,
      "ci_lower": 0.9343419673619417,
      "ci_upper": 0.9456580326380581
    },
    {
      "model": "gpt-4.1",
      "tone": "neutral",
      "dataset": "mmlu_stem",
      "mean_acc": 0.8133333333333334,
      "std_acc": 0.010408329997330601,
      "n_trials": 3,
      "ci_95": 0.011778134166515627,
      "ci_lower": 0.8015551991668177,
      "ci_upper": 0.825111467499849
    },
    {
      "model": "gpt-4.1",
      "tone": "neutral",
      "dataset": "truthfulqa",
      "mean_acc": 0.8649999999999999,
      "std_acc": 0.004999999999999977,
      "n_trials": 3,
      "ci_95": 0.005658032638058306,
      "ci_lower": 0.8593419673619416,
      "ci_upper": 0.8706580326380582
    },
    {
      "model": "gpt-4.1",
      "tone": "polite",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.935,
      "std_acc": 0.004999999999999963,
      "n_trials": 3,
      "ci_95": 0.005658032638058291,
      "ci_lower": 0.9293419673619417,
      "ci_upper": 0.9406580326380584
    },
    {
      "model": "gpt-4.1",
      "tone": "polite",
      "dataset": "mmlu_stem",
      "mean_acc": 0.8266666666666667,
      "std_acc": 0.007637626158259719,
      "n_trials": 3,
      "ci_95": 0.008642787616144313,
      "ci_lower": 0.8180238790505223,
      "ci_upper": 0.835309454282811
    },
    {
      "model": "gpt-4.1",
      "tone": "polite",
      "dataset": "truthfulqa",
      "mean_acc": 0.8533333333333334,
      "std_acc": 0.007637626158259758,
      "n_trials": 3,
      "ci_95": 0.008642787616144358,
      "ci_lower": 0.844690545717189,
      "ci_upper": 0.8619761209494777
    },
    {
      "model": "gpt-4.1",
      "tone": "rude",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.9266666666666667,
      "std_acc": 0.0028867513459480834,
      "n_trials": 3,
      "ci_95": 0.0032666666666666157,
      "ci_lower": 0.9234000000000001,
      "ci_upper": 0.9299333333333334
    },
    {
      "model": "gpt-4.1",
      "tone": "rude",
      "dataset": "mmlu_stem",
      "mean_acc": 0.8116666666666666,
      "std_acc": 0.002886751345948059,
      "n_trials": 3,
      "ci_95": 0.003266666666666588,
      "ci_lower": 0.8084,
      "ci_upper": 0.8149333333333333
    },
    {
      "model": "gpt-4.1",
      "tone": "rude",
      "dataset": "truthfulqa",
      "mean_acc": 0.87,
      "std_acc": 0.005000000000000032,
      "n_trials": 3,
      "ci_95": 0.005658032638058369,
      "ci_lower": 0.8643419673619416,
      "ci_upper": 0.8756580326380584
    },
    {
      "model": "gpt-4.1",
      "tone": "very_polite",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.9266666666666667,
      "std_acc": 0.0028867513459480834,
      "n_trials": 3,
      "ci_95": 0.0032666666666666157,
      "ci_lower": 0.9234000000000001,
      "ci_upper": 0.9299333333333334
    },
    {
      "model": "gpt-4.1",
      "tone": "very_polite",
      "dataset": "mmlu_stem",
      "mean_acc": 0.8316666666666667,
      "std_acc": 0.0028867513459481715,
      "n_trials": 3,
      "ci_95": 0.0032666666666667154,
      "ci_lower": 0.8283999999999999,
      "ci_upper": 0.8349333333333334
    },
    {
      "model": "gpt-4.1",
      "tone": "very_polite",
      "dataset": "truthfulqa",
      "mean_acc": 0.8583333333333334,
      "std_acc": 0.0028867513459481233,
      "n_trials": 3,
      "ci_95": 0.0032666666666666608,
      "ci_lower": 0.8550666666666668,
      "ci_upper": 0.8616
    },
    {
      "model": "gpt-4.1",
      "tone": "very_rude",
      "dataset": "mmlu_humanities",
      "mean_acc": 0.9333333333333335,
      "std_acc": 0.0028867513459481316,
      "n_trials": 3,
      "ci_95": 0.00326666666666667,
      "ci_lower": 0.9300666666666668,
      "ci_upper": 0.9366000000000001
    },
    {
      "model": "gpt-4.1",
      "tone": "very_rude",
      "dataset": "mmlu_stem",
      "mean_acc": 0.8249999999999998,
      "std_acc": 0.0050000000000000044,
      "n_trials": 3,
      "ci_95": 0.005658032638058337,
      "ci_lower": 0.8193419673619415,
      "ci_upper": 0.8306580326380582
    },
    {
      "model": "gpt-4.1",
      "tone": "very_rude",
      "dataset": "truthfulqa",
      "mean_acc": 0.8566666666666666,
      "std_acc": 0.005773502691896279,
      "n_trials": 3,
      "ci_95": 0.006533333333333357,
      "ci_lower": 0.8501333333333332,
      "ci_upper": 0.8632
    }
  ],
  "effect_sizes": [
    {
      "model": "gpt-4.1",
      "dataset": "mmlu_stem",
      "tone": "very_polite",
      "baseline": "neutral",
      "cohens_d": 2.40039679259591,
      "t_stat": 2.750000000000004,
      "p_value": 0.11070270820011216,
      "tone_mean": 0.8316666666666667,
      "baseline_mean": 0.8133333333333335,
      "diff": 0.0183333333333332,
      "p_corrected": 1.0
    },
    {
      "model": "gpt-4.1",
      "dataset": "mmlu_stem",
      "tone": "polite",
      "baseline": "neutral",
      "cohens_d": 1.4605934866804327,
      "t_stat": 8.000000000000133,
      "p_value": 0.015268072165337642,
      "tone_mean": 0.8266666666666667,
      "baseline_mean": 0.8133333333333335,
      "diff": 0.013333333333333197,
      "p_corrected": 0.44277409279479163
    },
    {
      "model": "gpt-4.1",
      "dataset": "mmlu_stem",
      "tone": "rude",
      "baseline": "neutral",
      "cohens_d": -0.21821789023601332,
      "t_stat": -0.22941573387056377,
      "p_value": 0.8398718461949115,
      "tone_mean": 0.8116666666666666,
      "baseline_mean": 0.8133333333333335,
      "diff": -0.0016666666666668162,
      "p_corrected": 1.0
    },
    {
      "model": "gpt-4.1",
      "dataset": "mmlu_stem",
      "tone": "very_rude",
      "baseline": "neutral",
      "cohens_d": 1.4288690166234912,
      "t_stat": 1.941450686788301,
      "p_value": 0.19170962313452408,
      "tone_mean": 0.8249999999999998,
      "baseline_mean": 0.8133333333333335,
      "diff": 0.011666666666666381,
      "p_corrected": 1.0
    },
    {
      "model": "gpt-4.1",
      "dataset": "mmlu_stem",
      "tone": "emotion_positive",
      "baseline": "neutral",
      "cohens_d": 1.745743121887915,
      "t_stat": 2.0,
      "p_value": 0.18350341907227397,
      "tone_mean": 0.8266666666666665,
      "baseline_mean": 0.8133333333333335,
      "diff": 0.013333333333333086,
      "p_corrected": 1.0
    },
    {
      "model": "gpt-4.1",
      "dataset": "mmlu_stem",
      "tone": "emotion_negative",
      "baseline": "neutral",
      "cohens_d": -1.3191415199864849e-14,
      "t_stat": 0.0,
      "p_value": 1.0,
      "tone_mean": 0.8133333333333334,
      "baseline_mean": 0.8133333333333335,
      "diff": -1.1102230246251565e-16,
      "p_corrected": 1.0
    },
    {
      "model": "gpt-4.1",
      "dataset": "mmlu_humanities",
      "tone": "very_polite",
      "baseline": "neutral",
      "cohens_d": -3.265986323710895,
      "t_stat": -8.000000000000133,
      "p_value": 0.015268072165337642,
      "tone_mean": 0.9266666666666667,
      "baseline_mean": 0.94,
      "diff": -0.013333333333333197,
      "p_corrected": 0.44277409279479163
    },
    {
      "model": "gpt-4.1",
      "dataset": "mmlu_humanities",
      "tone": "polite",
      "baseline": "neutral",
      "cohens_d": -0.9999999999999889,
      "t_stat": -1.7320508075688839,
      "p_value": 0.22540333075851543,
      "tone_mean": 0.935,
      "baseline_mean": 0.94,
      "diff": -0.004999999999999893,
      "p_corrected": 1.0
    },
    {
      "model": "gpt-4.1",
      "dataset": "mmlu_humanities",
      "tone": "rude",
      "baseline": "neutral",
      "cohens_d": -3.265986323710895,
      "t_stat": -8.000000000000133,
      "p_value": 0.015268072165337642,
      "tone_mean": 0.9266666666666667,
      "baseline_mean": 0.94,
      "diff": -0.013333333333333197,
      "p_corrected": 0.44277409279479163
    },
    {
      "model": "gpt-4.1",
      "dataset": "mmlu_humanities",
      "tone": "very_rude",
      "baseline": "neutral",
      "cohens_d": -1.63299316185542,
      "t_stat": -1.9999999999999998,
      "p_value": 0.183503419072274,
      "tone_mean": 0.9333333333333335,
      "baseline_mean": 0.94,
      "diff": -0.0066666666666664876,
      "p_corrected": 1.0
    },
    {
      "model": "gpt-4.1",
      "dataset": "mmlu_humanities",
      "tone": "emotion_positive",
      "baseline": "neutral",
      "cohens_d": -0.8164965809277284,
      "t_stat": -0.7559289460184581,
      "p_value": 0.5285954792089667,
      "tone_mean": 0.9366666666666666,
      "baseline_mean": 0.94,
      "diff": -0.0033333333333332993,
      "p_corrected": 1.0
    },
    {
      "model": "gpt-4.1",
      "dataset": "mmlu_humanities",
      "tone": "emotion_negative",
      "baseline": "neutral",
      "cohens_d": -0.9999999999999889,
      "t_stat": -1.7320508075688839,
      "p_value": 0.22540333075851543,
      "tone_mean": 0.935,
      "baseline_mean": 0.94,
      "diff": -0.004999999999999893,
      "p_corrected": 1.0
    },
    {
      "model": "gpt-4.1",
      "dataset": "truthfulqa",
      "tone": "very_polite",
      "baseline": "neutral",
      "cohens_d": -1.6329931618554339,
      "t_stat": -2.0,
      "p_value": 0.18350341907227397,
      "tone_mean": 0.8583333333333333,
      "baseline_mean": 0.8649999999999999,
      "diff": -0.006666666666666599,
      "p_corrected": 1.0
    },
    {
      "model": "gpt-4.1",
      "dataset": "truthfulqa",
      "tone": "polite",
      "baseline": "neutral",
      "cohens_d": -1.8073922282300992,
      "t_stat": -2.6457513110645903,
      "p_value": 0.11808289631180315,
      "tone_mean": 0.8533333333333334,
      "baseline_mean": 0.8649999999999999,
      "diff": -0.011666666666666492,
      "p_corrected": 1.0
    },
    {
      "model": "gpt-4.1",
      "dataset": "truthfulqa",
      "tone": "rude",
      "baseline": "neutral",
      "cohens_d": 1.0000000000000444,
      "t_stat": Infinity,
      "p_value": 0.0,
      "tone_mean": 0.8700000000000001,
      "baseline_mean": 0.8649999999999999,
      "diff": 0.0050000000000002265,
      "p_corrected": 0.0
    },
    {
      "model": "gpt-4.1",
      "dataset": "truthfulqa",
      "tone": "very_rude",
      "baseline": "neutral",
      "cohens_d": -1.5430334996209123,
      "t_stat": -5.0,
      "p_value": 0.037749551350623724,
      "tone_mean": 0.8566666666666666,
      "baseline_mean": 0.8649999999999999,
      "diff": -0.008333333333333304,
      "p_corrected": 0.9437387837655931
    },
    {
      "model": "gpt-4.1",
      "dataset": "truthfulqa",
      "tone": "emotion_positive",
      "baseline": "neutral",
      "cohens_d": -3.265986323710895,
      "t_stat": -3.0237157840738176,
      "p_value": 0.09417837268432341,
      "tone_mean": 0.8516666666666666,
      "baseline_mean": 0.8649999999999999,
      "diff": -0.013333333333333308,
      "p_corrected": 1.0
    },
    {
      "model": "gpt-4.1",
      "dataset": "truthfulqa",
      "tone": "emotion_negative",
      "baseline": "neutral",
      "cohens_d": -4.242640687119254,
      "t_stat": -5.196152422706631,
      "p_value": 0.035098718645984656,
      "tone_mean": 0.85,
      "baseline_mean": 0.8649999999999999,
      "diff": -0.014999999999999902,
      "p_corrected": 0.9125666847956011
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "mmlu_stem",
      "tone": "very_polite",
      "baseline": "neutral",
      "cohens_d": -5.0,
      "t_stat": -18.898223650461336,
      "p_value": 0.0027882946124364026,
      "tone_mean": 0.625,
      "baseline_mean": 0.7083333333333334,
      "diff": -0.08333333333333337,
      "p_corrected": 0.1087434898850197
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "mmlu_stem",
      "tone": "polite",
      "baseline": "neutral",
      "cohens_d": -5.0,
      "t_stat": -9.999999999999966,
      "p_value": 0.009852457023325756,
      "tone_mean": 0.6750000000000002,
      "baseline_mean": 0.7083333333333334,
      "diff": -0.033333333333333215,
      "p_corrected": 0.3152786247464221
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "mmlu_stem",
      "tone": "rude",
      "baseline": "neutral",
      "cohens_d": 5.0,
      "t_stat": 36.499999999999964,
      "p_value": 0.0007497658036287885,
      "tone_mean": 0.83,
      "baseline_mean": 0.7083333333333334,
      "diff": 0.12166666666666659,
      "p_corrected": 0.02999063214515154
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "mmlu_stem",
      "tone": "very_rude",
      "baseline": "neutral",
      "cohens_d": 5.0,
      "t_stat": 57.99999999999999,
      "p_value": 0.0002971326762963116,
      "tone_mean": 0.805,
      "baseline_mean": 0.7083333333333334,
      "diff": 0.09666666666666668,
      "p_corrected": 0.012182439728148777
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "mmlu_stem",
      "tone": "emotion_positive",
      "baseline": "neutral",
      "cohens_d": -5.0,
      "t_stat": -84.99999999999991,
      "p_value": 0.00013837957583720474,
      "tone_mean": 0.425,
      "baseline_mean": 0.7083333333333334,
      "diff": -0.2833333333333334,
      "p_corrected": 0.005811942185162599
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "mmlu_stem",
      "tone": "emotion_negative",
      "baseline": "neutral",
      "cohens_d": 0.3086066999241701,
      "t_stat": 0.9999999999999999,
      "p_value": 0.4226497308103743,
      "tone_mean": 0.71,
      "baseline_mean": 0.7083333333333334,
      "diff": 0.0016666666666665941,
      "p_corrected": 1.0
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "mmlu_humanities",
      "tone": "very_polite",
      "baseline": "neutral",
      "cohens_d": 0.8164965809277441,
      "t_stat": 0.9999999999999999,
      "p_value": 0.4226497308103743,
      "tone_mean": 0.9466666666666667,
      "baseline_mean": 0.945,
      "diff": 0.0016666666666667052,
      "p_corrected": 1.0
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "mmlu_humanities",
      "tone": "polite",
      "baseline": "neutral",
      "cohens_d": 0.8164965809277441,
      "t_stat": 0.9999999999999999,
      "p_value": 0.4226497308103743,
      "tone_mean": 0.9466666666666667,
      "baseline_mean": 0.945,
      "diff": 0.0016666666666667052,
      "p_corrected": 1.0
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "mmlu_humanities",
      "tone": "rude",
      "baseline": "neutral",
      "cohens_d": 2.0000000000000018,
      "t_stat": Infinity,
      "p_value": 0.0,
      "tone_mean": 0.955,
      "baseline_mean": 0.945,
      "diff": 0.010000000000000009,
      "p_corrected": 0.0
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "mmlu_humanities",
      "tone": "very_rude",
      "baseline": "neutral",
      "cohens_d": 5.0,
      "t_stat": Infinity,
      "p_value": 0.0,
      "tone_mean": 0.9499999999999998,
      "baseline_mean": 0.945,
      "diff": 0.004999999999999893,
      "p_corrected": 0.0
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "mmlu_humanities",
      "tone": "emotion_positive",
      "baseline": "neutral",
      "cohens_d": -5.0,
      "t_stat": -129.9999999999999,
      "p_value": 5.916634623407341e-05,
      "tone_mean": 0.7283333333333334,
      "baseline_mean": 0.945,
      "diff": -0.21666666666666656,
      "p_corrected": 0.0025441528880651566
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "mmlu_humanities",
      "tone": "emotion_negative",
      "baseline": "neutral",
      "cohens_d": 4.082482904638612,
      "t_stat": 5.0,
      "p_value": 0.037749551350623724,
      "tone_mean": 0.9533333333333333,
      "baseline_mean": 0.945,
      "diff": 0.008333333333333304,
      "p_corrected": 0.9437387837655931
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "truthfulqa",
      "tone": "very_polite",
      "baseline": "neutral",
      "cohens_d": -4.999999999999982,
      "t_stat": -Infinity,
      "p_value": 0.0,
      "tone_mean": 0.935,
      "baseline_mean": 0.96,
      "diff": -0.02499999999999991,
      "p_corrected": 0.0
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "truthfulqa",
      "tone": "polite",
      "baseline": "neutral",
      "cohens_d": -5.0,
      "t_stat": -16.999999999999936,
      "p_value": 0.0034423510070664388,
      "tone_mean": 0.9316666666666668,
      "baseline_mean": 0.96,
      "diff": -0.02833333333333321,
      "p_corrected": 0.13080933826852467
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "truthfulqa",
      "tone": "rude",
      "baseline": "neutral",
      "cohens_d": -5.0,
      "t_stat": -10.0,
      "p_value": 0.00985245702332569,
      "tone_mean": 0.9433333333333332,
      "baseline_mean": 0.96,
      "diff": -0.01666666666666672,
      "p_corrected": 0.3152786247464221
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "truthfulqa",
      "tone": "very_rude",
      "baseline": "neutral",
      "cohens_d": -3.0000000000000027,
      "t_stat": -Infinity,
      "p_value": 0.0,
      "tone_mean": 0.945,
      "baseline_mean": 0.96,
      "diff": -0.015000000000000013,
      "p_corrected": 0.0
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "truthfulqa",
      "tone": "emotion_positive",
      "baseline": "neutral",
      "cohens_d": -5.0,
      "t_stat": -13.000000000000266,
      "p_value": 0.005865153227565443,
      "tone_mean": 0.9383333333333334,
      "baseline_mean": 0.96,
      "diff": -0.021666666666666612,
      "p_corrected": 0.19355005650965962
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "truthfulqa",
      "tone": "emotion_negative",
      "baseline": "neutral",
      "cohens_d": -1.0000000000000009,
      "t_stat": -Infinity,
      "p_value": 0.0,
      "tone_mean": 0.955,
      "baseline_mean": 0.96,
      "diff": -0.0050000000000000044,
      "p_corrected": 0.0
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "mmlu_stem",
      "tone": "very_polite",
      "baseline": "neutral",
      "cohens_d": 5.0,
      "t_stat": 14.000000000000002,
      "p_value": 0.005063323673817972,
      "tone_mean": 0.8883333333333333,
      "baseline_mean": 0.8649999999999999,
      "diff": 0.023333333333333428,
      "p_corrected": 0.18734297593126495
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "mmlu_stem",
      "tone": "polite",
      "baseline": "neutral",
      "cohens_d": 2.8284271247462214,
      "t_stat": 3.4641016151377544,
      "p_value": 0.07417990022744855,
      "tone_mean": 0.875,
      "baseline_mean": 0.8649999999999999,
      "diff": 0.01000000000000012,
      "p_corrected": 1.0
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "mmlu_stem",
      "tone": "rude",
      "baseline": "neutral",
      "cohens_d": 5.0,
      "t_stat": Infinity,
      "p_value": 0.0,
      "tone_mean": 0.88,
      "baseline_mean": 0.8649999999999999,
      "diff": 0.015000000000000124,
      "p_corrected": 0.0
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "mmlu_stem",
      "tone": "very_rude",
      "baseline": "neutral",
      "cohens_d": 5.0,
      "t_stat": 14.000000000000002,
      "p_value": 0.005063323673817972,
      "tone_mean": 0.8883333333333333,
      "baseline_mean": 0.8649999999999999,
      "diff": 0.023333333333333428,
      "p_corrected": 0.18734297593126495
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "mmlu_stem",
      "tone": "emotion_positive",
      "baseline": "neutral",
      "cohens_d": 5.0,
      "t_stat": 10.0,
      "p_value": 0.00985245702332569,
      "tone_mean": 0.8816666666666667,
      "baseline_mean": 0.8649999999999999,
      "diff": 0.01666666666666683,
      "p_corrected": 0.3152786247464221
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "mmlu_stem",
      "tone": "emotion_negative",
      "baseline": "neutral",
      "cohens_d": 5.0,
      "t_stat": Infinity,
      "p_value": 0.0,
      "tone_mean": 0.88,
      "baseline_mean": 0.8649999999999999,
      "diff": 0.015000000000000124,
      "p_corrected": 0.0
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "mmlu_humanities",
      "tone": "very_polite",
      "baseline": "neutral",
      "cohens_d": -0.40824829046384487,
      "t_stat": -0.5,
      "p_value": 0.6666666666666667,
      "tone_mean": 0.89,
      "baseline_mean": 0.8916666666666666,
      "diff": -0.0016666666666665941,
      "p_corrected": 1.0
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "mmlu_humanities",
      "tone": "polite",
      "baseline": "neutral",
      "cohens_d": -1.6329931618554065,
      "t_stat": -2.0,
      "p_value": 0.18350341907227397,
      "tone_mean": 0.8850000000000001,
      "baseline_mean": 0.8916666666666666,
      "diff": -0.0066666666666664876,
      "p_corrected": 1.0
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "mmlu_humanities",
      "tone": "rude",
      "baseline": "neutral",
      "cohens_d": 0.8164965809277441,
      "t_stat": 0.9999999999999999,
      "p_value": 0.4226497308103743,
      "tone_mean": 0.895,
      "baseline_mean": 0.8916666666666666,
      "diff": 0.0033333333333334103,
      "p_corrected": 1.0
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "mmlu_humanities",
      "tone": "very_rude",
      "baseline": "neutral",
      "cohens_d": -0.40824829046384487,
      "t_stat": -0.5,
      "p_value": 0.6666666666666667,
      "tone_mean": 0.89,
      "baseline_mean": 0.8916666666666666,
      "diff": -0.0016666666666665941,
      "p_corrected": 1.0
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "mmlu_humanities",
      "tone": "emotion_positive",
      "baseline": "neutral",
      "cohens_d": 2.041241452319333,
      "t_stat": 2.5,
      "p_value": 0.12961172022151082,
      "tone_mean": 0.9,
      "baseline_mean": 0.8916666666666666,
      "diff": 0.008333333333333415,
      "p_corrected": 1.0
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "mmlu_humanities",
      "tone": "emotion_negative",
      "baseline": "neutral",
      "cohens_d": -0.73854894587598,
      "t_stat": -1.0,
      "p_value": 0.4226497308103742,
      "tone_mean": 0.8866666666666667,
      "baseline_mean": 0.8916666666666666,
      "diff": -0.004999999999999893,
      "p_corrected": 1.0
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "truthfulqa",
      "tone": "very_polite",
      "baseline": "neutral",
      "cohens_d": -5.0,
      "t_stat": -14.000000000000002,
      "p_value": 0.005063323673817972,
      "tone_mean": 0.8216666666666667,
      "baseline_mean": 0.8450000000000001,
      "diff": -0.023333333333333428,
      "p_corrected": 0.18734297593126495
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "truthfulqa",
      "tone": "polite",
      "baseline": "neutral",
      "cohens_d": -5.0,
      "t_stat": -Infinity,
      "p_value": 0.0,
      "tone_mean": 0.815,
      "baseline_mean": 0.8450000000000001,
      "diff": -0.030000000000000138,
      "p_corrected": 0.0
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "truthfulqa",
      "tone": "rude",
      "baseline": "neutral",
      "cohens_d": 5.0,
      "t_stat": Infinity,
      "p_value": 0.0,
      "tone_mean": 0.855,
      "baseline_mean": 0.8450000000000001,
      "diff": 0.009999999999999898,
      "p_corrected": 0.0
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "truthfulqa",
      "tone": "very_rude",
      "baseline": "neutral",
      "cohens_d": -3.2659863237109765,
      "t_stat": -4.0,
      "p_value": 0.05719095841793663,
      "tone_mean": 0.8383333333333333,
      "baseline_mean": 0.8450000000000001,
      "diff": -0.006666666666666821,
      "p_corrected": 1.0
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "truthfulqa",
      "tone": "emotion_positive",
      "baseline": "neutral",
      "cohens_d": 5.0,
      "t_stat": 14.000000000000002,
      "p_value": 0.005063323673817972,
      "tone_mean": 0.8683333333333333,
      "baseline_mean": 0.8450000000000001,
      "diff": 0.023333333333333206,
      "p_corrected": 0.18734297593126495
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "truthfulqa",
      "tone": "emotion_negative",
      "baseline": "neutral",
      "cohens_d": 5.0,
      "t_stat": Infinity,
      "p_value": 0.0,
      "tone_mean": 0.875,
      "baseline_mean": 0.8450000000000001,
      "diff": 0.029999999999999916,
      "p_corrected": 0.0
    }
  ],
  "anova_results": [
    {
      "model": "gpt-4.1",
      "dataset": "mmlu_stem",
      "F_stat": 5.577777777777781,
      "p_value": 0.0038550492897100773,
      "eta_squared": 0.7050561797752783,
      "n_conditions": 7
    },
    {
      "model": "gpt-4.1",
      "dataset": "mmlu_humanities",
      "F_stat": 4.8461538461538884,
      "p_value": 0.007078548983474476,
      "eta_squared": 0.6749999999999954,
      "n_conditions": 7
    },
    {
      "model": "gpt-4.1",
      "dataset": "truthfulqa",
      "F_stat": 7.087719298245618,
      "p_value": 0.0012641917211346756,
      "eta_squared": 0.7523277467411582,
      "n_conditions": 7
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "mmlu_stem",
      "F_stat": 2384.1578947371045,
      "p_value": 3.069571131284911e-20,
      "eta_squared": 0.9990222744982724,
      "n_conditions": 7
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "mmlu_humanities",
      "F_stat": 4408.6666666653155,
      "p_value": 4.168487163214296e-22,
      "eta_squared": 0.9994710194211432,
      "n_conditions": 7
    },
    {
      "model": "claude-sonnet-4.5",
      "dataset": "truthfulqa",
      "F_stat": 90.11111111111228,
      "p_value": 2.2468791405514803e-10,
      "eta_squared": 0.9747596153846138,
      "n_conditions": 7
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "mmlu_stem",
      "F_stat": 27.388888888888868,
      "p_value": 5.738463337236675e-07,
      "eta_squared": 0.9214953271028088,
      "n_conditions": 7
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "mmlu_humanities",
      "F_stat": 5.878787878787878,
      "p_value": 0.003043898783263235,
      "eta_squared": 0.7158671586715786,
      "n_conditions": 7
    },
    {
      "model": "gemini-2.5-flash",
      "dataset": "truthfulqa",
      "F_stat": 424.44444444444775,
      "p_value": 5.206697542823962e-15,
      "eta_squared": 0.9945326737828681,
      "n_conditions": 7
    }
  ],
  "overall": {
    "mean_effect_size": -0.13703520094718907,
    "median_effect_size": -0.40824829046384487,
    "max_abs_effect": 5.0,
    "n_significant": 31,
    "n_total_comparisons": 54,
    "pct_significant": 57.407407407407405
  }
}